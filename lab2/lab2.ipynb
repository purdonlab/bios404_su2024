{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hidden Markov Model\n",
    "author: Tom Stone <tomstone@stanford.edu>\\\n",
    "License: BSD (3-clause)\n",
    "\n",
    "We will use Numpy for our numerical computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects used consistently throughout the functions:\n",
    "\n",
    "The state at time $t$ is given by $q_t$, and the set of all states is $\\mathbf{Q}$. The state transition matrix is denoted $A = [a_{ij}]_{i,j=1}^{N,N}$ where $a_{ij} = \\mathbb{P}[q_{t+1}=j|q_t=i]$. The observation at time $t$ is given by $O_t$, and the set of all observations is $\\mathbf{O}$. The emission probabilities are $b_j(O_t) = \\mathbb{P}[O_t|q_t=j]$. The prior probability of the initial states are $\\pi = \\{\\pi_i\\}$. The set of model parameters is $\\lambda$.\n",
    "<!-- \n",
    "POTENTIALLY CONFUSING SIDENOTE:\n",
    "It turns out that for the somata implementation it is more useful to think of the $b$ matrix as being $N\\times T$ instead of $N\\times M$ (where $M$ is the number of states), because the only emission probability you use at time $t$ is $b_j(O_t)$, you don't use any other $b_j(m)$ for that time. -->\n",
    "# Problem 1: Observable Markov Model\n",
    "\n",
    "State 1: precipitation\\\n",
    "State 2: Cloudy\\\n",
    "State 3: Sunny\n",
    "\n",
    "Transition matrix:\n",
    "\\begin{align*}\n",
    "a_{11}\\hspace{1mm} a_{12}\\hspace{1mm} a_{13}\\\\\n",
    "a_{21}\\hspace{1mm} a_{22}\\hspace{1mm} a_{23}\\\\\n",
    "a_{31}\\hspace{1mm} a_{32}\\hspace{1mm} a_{33}\n",
    "\\end{align*}\n",
    "\n",
    "Q1: given that today is sunny, what is the probability the next 7 days will be sunny, but the 8th won't?\n",
    "\n",
    "Q2: Given that today is sunny, what is the *expected* number of consecutive days it will be sunny?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Problem 2: The forward-backward algorithm(s)\n",
    "\n",
    "Q3: A pen and paper implementation of the forward & backward algorithms\n",
    "\n",
    "## The discrete-valued HMM forward algorithm\n",
    "\n",
    "Want to calculate\n",
    "$$\\mathbb{P}[\\mathbf{O}=O_1O_2...O_T|\\lambda]$$\n",
    "\n",
    "We use the inductive value $\\alpha_t(i)$\n",
    "$$\\alpha_t(i) = \\mathbb{P}[O_1O_2...O_t,q_t = S_i|\\lambda]$$\n",
    "\n",
    "Which is initialized as \n",
    "$$\\forall\\text{ (forall) } j\\hspace{1mm}\\alpha_1(j) = \\pi_jb_j(O_1)$$\n",
    "where $\\pi_j$ is the prior on state $j$, $b_j(O_1)$ is the probability of observing $O_1$ given state $j$, and $A=[a_{ij}]$ is the state transition matrix.\n",
    "\n",
    "The inductive step:\n",
    "\n",
    "$$\\alpha_{t+1}(j) = \\left[\\sum_{i=0}^{N-1}\\alpha_t(i)a_{ij}\\right]b_j(O_{t+1})$$\n",
    "\n",
    "Termination:\n",
    "\n",
    "$$\\mathbb{P}[\\mathbf{O}|\\lambda]=\\sum_{i=0}^{N-1}\\alpha_T(i)$$\n",
    "\n",
    "### Function `forward(A, b, pi, obs)`\n",
    "\n",
    "Your task is to write a function that takes in the state transition matrix `A`, emission probabilities `b`, state priors `pi`, and observations `obs`, and returns the likelihood $\\mathbb{P}[\\mathbf{O}|\\lambda]$ calculated with the forward algorithm, as well as the $\\alpha_t$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(A, b, pi, obs):\n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        A: NxN transition matrix, rows sum to 1\n",
    "        b: NxM emission probabilities, rows sum to 1\n",
    "        pi: N priors on hidden states\n",
    "        obs: T observations\n",
    "    returns:\n",
    "        likelihood: likelihood P[O|lambda]\n",
    "        alpha: recursive object calculated in forward algorithm\n",
    "    \"\"\"\n",
    "    #############\n",
    "    # sanity checks for data shapes\n",
    "    #\n",
    "    N = A.shape[0] # number of states\n",
    "    assert N == A.shape[1], \"A is not square\"\n",
    "    assert N == b.shape[0], \"b dimension does not match A\"\n",
    "    assert N == pi.shape[0], \"pi dimension does not match A\"\n",
    "\n",
    "    M = b.shape[1]\n",
    "    T = len(obs)\n",
    "    #\n",
    "    #############\n",
    "\n",
    "    # step 1: \n",
    "    # preallocate alpha matrix\n",
    "    alpha = np.zeros((N,T))\n",
    "\n",
    "\n",
    "    # step 1.1: \n",
    "    # initialize alpha\n",
    "    alpha[:,0] = pi * b[:,obs[0]]\n",
    "\n",
    "\n",
    "    # step 2: induction \n",
    "    for t in range(1,T):\n",
    "        # fill in here\n",
    "        # can do induction with another loop\n",
    "        # or with a careful matrix multiplication\n",
    "        \n",
    "        for j in range(M):\n",
    "            # solution #1\n",
    "            alpha[j,t] = sum([al * a for (al,a) in zip(alpha[:,t-1], A[:,j])]) * b[j,obs[t]]\n",
    "\n",
    "            # solution #2\n",
    "            alpha[j,t] = np.dot(alpha[:,t-1], A[:,j]) * b[j,obs[t]]\n",
    "\n",
    "        # solution #3\n",
    "        alpha[:,t] = (np.transpose(A) @ alpha[:,t-1]) * b[:, obs[t]]\n",
    "    \n",
    "\n",
    "    # step 3: termination\n",
    "    likelihood = sum(alpha[:,-1])\n",
    "\n",
    "\n",
    "    return likelihood, alpha\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your paper calculations\n",
    "forward(\n",
    "    np.array([[0.9, 0.1], [0.2, 0.8]]), # A\n",
    "    np.array([[0.7, 0.3], [0.4, 0.6]]), # b\n",
    "    np.array([0.5, 0.5]),               # pi\n",
    "    np.array([0,0,1,0])                 # obs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete-valued HMM backward algorithm\n",
    "\n",
    "Again, we want to calculate\n",
    "$$\\mathbb{P}[\\mathbf{O}|\\lambda]$$\n",
    "\n",
    "The inductive value this time is:\n",
    "$$\\beta_t(i) = \\mathbb{P}[O_{t+1}...O_T|q_t=S_i,\\lambda]$$\n",
    "\n",
    "The base case is simpler than in the $\\alpha$ case:\n",
    "$$\\forall i\\hspace{1mm}\\beta_T(i) = 1$$\n",
    "\n",
    "But the recursive formula is just a tad more complicated (multiplication by $b$ is now on the inside)\n",
    "$$\\beta_t(i) = \\sum_{j=0}^{N-1}a_{ij}\\beta_{t+1}(j)b_j(O_{t+1})$$\n",
    "\n",
    "The termination is:\n",
    "$$\\mathbb{P}[\\mathbf{O}|\\lambda] = \\sum_{j=0}^{N-1}\\pi_j\\beta_1(j)b_j(O_1)$$\n",
    "\n",
    "### Function `backward(A, b, pi, obs)`\n",
    "\n",
    "Your task is to write a function that takes in the state transition matrix `A`, emission probabilities `b`, state priors `pi`, and observations `obs`, and returns the likelihood $\\mathbb{P}[\\mathbf{O}|\\lambda]$ computed with the backwards algorithm, as well as the $\\beta_t$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(A, b, pi, obs):\n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        A: NxN transition matrix, rows sum to 1\n",
    "        b: NxM emission probabilities, rows sum to 1\n",
    "        pi: N priors on states\n",
    "        obs: T observations\n",
    "    returns:\n",
    "        likelihood: likelihood P[O|lambda]\n",
    "        beta: recursive object calculated in backward algorithm\n",
    "    \"\"\"\n",
    "    #############\n",
    "    # sanity checks for data shapes\n",
    "    #\n",
    "    N = A.shape[0] # number of states\n",
    "    assert N == A.shape[1], \"A is not square\"\n",
    "    assert N == b.shape[0], \"b dimension does not match A\"\n",
    "    assert N == pi.shape[0], \"pi dimension does not match A\"\n",
    "\n",
    "    M = b.shape[1]\n",
    "    T = len(obs)\n",
    "    #\n",
    "    #############\n",
    "\n",
    "    # step 1: preallocate beta\n",
    "    beta = np.ones((N,T))\n",
    "\n",
    "\n",
    "    # step 1.1: initialize beta_T\n",
    "    # beta[:,-1] = 1 # don't need to do as we initialized beta as an array of ones\n",
    "        \n",
    "\n",
    "    # step 2: inductive step\n",
    "    for t in reversed(range(0,T-1)):\n",
    "        # fill in here\n",
    "        # can do induction with another loop\n",
    "        # or with a careful matrix multiplication\n",
    "        for i in range(M):\n",
    "            # solution #1\n",
    "            beta[i,t] = sum([a * bet * b for (a, bet, b) in zip(A[i,:], beta[:,t+1], b[:, obs[t+1]])])\n",
    "\n",
    "            # solution #2\n",
    "            beta[i,t] = np.dot(A[i,:], beta[:,t+1] * b[:,obs[t+1]])\n",
    "\n",
    "        # solution #3\n",
    "        beta[:,t] = A @ (beta[:,t+1] * b[:,obs[t+1]])\n",
    "\n",
    "\n",
    "\n",
    "    # step 3: termination\n",
    "    likelihood = np.dot(pi * beta[:,0], b[:, obs[0]])\n",
    "\n",
    "\n",
    "\n",
    "    return likelihood, beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward(\n",
    "    np.array([[0.9, 0.1], [0.2, 0.8]]), # A\n",
    "    np.array([[0.7, 0.3], [0.4, 0.6]]), # b\n",
    "    np.array([0.5, 0.5]),               # pi\n",
    "    np.array([0,0,1,0])                 # obs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Optimal state estimation\n",
    "## Locally optimal state estimation\n",
    "As we discussed in class, there are several ways to define the 'optimal' state sequence. We will start with the classes that are individually most likely.\n",
    "\n",
    "The recursive object calculated here is;\n",
    "\n",
    "$$\\gamma_t(i) = \\mathbb{P}[q_t=S_i|\\mathbf{O},\\lambda]$$\n",
    "\n",
    "Rewriting using Bayes' rule, \n",
    "\n",
    "$$\\gamma_t(i) = \\frac{\\mathbb{P}[q_t=S_i,\\mathbf{O}|\\lambda]}{\\mathbb{P}[\\mathbf{O}|\\lambda]} = \\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{i=0}^{N-1} \\alpha_t(i)\\beta_t(i)}$$\n",
    "\n",
    "The most likely state at time $t$ is then:\n",
    "$$q_t^* = \\argmax_{0\\leq i\\leq N-1}[\\gamma_i(t)]$$\n",
    "\n",
    "### function `locally_optimal_state_sequence(A, b, pi, obs)`\n",
    "\n",
    "Your task is to write a function that takes in the state transition matrix `A`, emission probabilities `b`, state priors `pi`, and observations `obs`, and returns the sequence of states that are individually optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locally_optimal_state_sequence(A, b, pi, obs):\n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        A: NxN transition matrix, rows sum to 1\n",
    "        b: NxM emission probabilities, rows sum to 1\n",
    "        pi: N priors on states\n",
    "        obs: T observations\n",
    "    returns:\n",
    "        maxs: sequence of locally optimal states\n",
    "        gamma: recursive object calculated in locally optimal algorithm\n",
    "    \"\"\"\n",
    "    # get alpha, beta\n",
    "    _, alpha = forward(A, b, pi, obs)\n",
    "    _, beta = backward(A, b, pi, obs)\n",
    "\n",
    "\n",
    "    # compute gamma\n",
    "    gamma = alpha * beta\n",
    "\n",
    "    # normalize columns\n",
    "    tgamma = gamma.sum(0, keepdims=True)\n",
    "    gamma /= tgamma\n",
    "\n",
    "\n",
    "    # get argmax's\n",
    "    maxs = np.argmax(gamma, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "    return maxs, gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locally_optimal_state_sequence(\n",
    "    np.array([[0.9, 0.1], [0.2, 0.8]]), # A\n",
    "    np.array([[0.7, 0.3], [0.4, 0.6]]), # b\n",
    "    np.array([0.5, 0.5]), # pi\n",
    "    np.array([0,0,1,0]) # obs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globally optimal state estimation\n",
    "\n",
    "If you want to optimize $\\mathbb{P}[\\mathbf{Q}|\\mathbf{O},\\lambda]$ over $\\mathbf{Q}$, by Bayes rule it is equivalent to optimizing $\\mathbb{P}[\\mathbf{Q, O}|\\lambda]$\n",
    "\n",
    "The recursive objects here are:\n",
    "\n",
    "$$\\delta_t(i) = \\max_{q_0...q_{t-1}}\\mathbb{P}[q_0...q_{t-1},q_t=i,O_0...O_t|\\lambda]\\cdot b_j(O_{t+1})$$\n",
    "$$\\psi_t(i) = \\argmax_{q_0...q_{t-1}}\\mathbb{P}[q_0...q_{t-1},q_t=i,O_0...O_t|\\lambda]$$\n",
    "\n",
    "The base case is:\n",
    "\n",
    "$$\\delta_0(i) = \\pi_ib_i(O_1)$$\n",
    "$$\\psi_0(i) = 0\\text{ (not actually used)}$$\n",
    "\n",
    "The recursive formula is:\n",
    "\n",
    "$$\\delta_t(j) = \\max_i[\\delta_{t-1}(i)a_{ij}]\\cdot b_j(O_t)$$\n",
    "$$\\psi_{t}(j) = \\argmax_{i}[\\delta_{t-1}(i)a_{ij}]$$\n",
    "\n",
    "The Termination is \n",
    "$$P^* = \\max_i[\\delta_{T-1}(i)]$$\n",
    "$$q_T^* = \\argmax_i[\\delta_{T-1}(i)]$$\n",
    "\n",
    "And then you backtrack along the path with the formula\n",
    "\n",
    "$$q_t^* = \\psi_{t+1}(q_{t+1}^*)$$\n",
    "\n",
    "### function `globally_optimal_state_sequence(A, b, pi, obs)`\n",
    "\n",
    "Your task is to write a function that takes in the state transition matrix `A`, emission probabilities `b`, state priors `pi`, and observations `obs`, and returns the probability of the sequence $P^*$, the sequence itself $q^*$, and $\\delta$, the recursive object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globally_optimal_state_sequence(A, b, pi, obs):\n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        A: NxN transition matrix, rows sum to 1\n",
    "        b: NxM emission probabilities, rows sum to 1\n",
    "        pi: N priors on states\n",
    "        obs: T observations\n",
    "    returns:\n",
    "        Pstar: probability of qstar sequence\n",
    "        qstar: optimal sequence of states\n",
    "        delta: recursive object calculated in globally optimal algorithm\n",
    "    \"\"\"\n",
    "    #############\n",
    "    # sanity checks for data shapes\n",
    "    #\n",
    "    N = A.shape[0] # number of states\n",
    "    assert N == A.shape[1], \"A is not square\"\n",
    "    assert N == b.shape[0], \"b dimension does not match A\"\n",
    "    assert N == pi.shape[0], \"pi dimension does not match A\"\n",
    "\n",
    "    M = b.shape[1]\n",
    "    T = len(obs)\n",
    "    #\n",
    "    #############\n",
    "    \n",
    "\n",
    "    # step 1: preallocation\n",
    "    delta = np.zeros((N, T))\n",
    "    delta[:,0] = pi * b[:, obs[0]]\n",
    "    \n",
    "    psi = np.zeros((N, T))\n",
    "\n",
    "\n",
    "    # step 2: recursion\n",
    "    for t in range(1,T):\n",
    "        for j in range(M):\n",
    "            # option 1:\n",
    "            temp = [d * a for (d,a) in zip(delta[:,t-1], A[:,j])]\n",
    "            # option 2:\n",
    "            temp = delta[:, t-1] * A[:,j]\n",
    "\n",
    "            delta[j,t] = np.max(temp) * b[j,obs[t]]\n",
    "            psi[j,t] = np.argmax(temp)\n",
    "\n",
    "        # # option 3:\n",
    "        temp = delta[:,t-1][:,None] * A # avoid numpy auto coercing to row vector??, thus messing up mult.\n",
    "        delta[:,t] = np.max(temp, axis=0) * b[:,obs[t]]\n",
    "        psi[:,t] = np.argmax(temp, axis=0)\n",
    "    \n",
    "\n",
    "    # step 3: termination, init. qstar\n",
    "    Pstar = np.max(delta[:,-1])\n",
    "    qstar = np.zeros((T), dtype=np.int64)\n",
    "    qstar[-1] = np.argmax(delta[:,-1])\n",
    "\n",
    "\n",
    "    # step 4: path backtracking\n",
    "    for t in reversed(range(0, T-1)):\n",
    "        qstar[t] = psi[qstar[t+1], t+1]\n",
    "\n",
    "\n",
    "    return Pstar, qstar, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globally_optimal_state_sequence(\n",
    "    np.array([[0.9, 0.1], [0.2, 0.8]]), # A\n",
    "    np.array([[0.7, 0.3], [0.4, 0.6]]), # b\n",
    "    np.array([0.5, 0.5]), # pi\n",
    "    np.array([0,0,1,0]) # obs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the coercion\n",
    "print(np.array([1,2])[:,None] * np.array([[1,2],[3,4]]))\n",
    "print(np.array([1,2]) * np.array([[1,2],[3,4]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
